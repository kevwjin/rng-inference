source .venv/bin/activate

# llama.cpp GGUF settings (adjust n_gpu_layers if needed)
export LLAMA_MODEL_PATH=/usr/share/ollama/.ollama/models/blobs/sha256-9c8a9ab5edab20fcfa0e9ca322f0131c3bfb2f5a2f4ec12425a761f2f12deefa
export LLAMA_N_GPU_LAYERS=26
export LLAMA_N_CTX=4096
export LLAMA_N_THREADS=0
