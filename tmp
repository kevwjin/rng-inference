setting the range from 0 to 255 results in the chinese prompt outputting significantly more often. the english prompt on the other hand has no problem, which is expected considering llama is from meta, an american company.

depending on the lstm architecture, the lstm could output numbers that it hasn't seen before. but typically, the lstm just has classes that it would output. i'm thinking of generating the training data so that all the numbers are outputted.
